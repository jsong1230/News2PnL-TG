"""ì˜¤ì „ ë¦¬í¬íŠ¸ ìƒì„± ëª¨ë“ˆ"""
import logging
from typing import List
from datetime import datetime
from collections import defaultdict

from src.config import (
    NEWS_PROVIDER, GOOGLE_NEWS_QUERY, GOOGLE_NEWS_QUERIES, GOOGLE_NEWS_MAX_PER_QUERY,
    NEWS_WINDOW_MODE, DEFAULT_NEWS_QUERIES, LLM_ENABLED, LLM_MODEL, NEWS_DEBUG_TAGS,
    OVERNIGHT_ENABLED, OVERNIGHT_DEBUG, OVERNIGHT_TICKERS
)
from src.news.provider import get_news_provider, DummyNewsProvider
from src.news.base import NewsItem
from src.analysis.news_analyzer import create_digest, NewsDigest
from src.analysis.stock_picker import pick_watch_stocks, WatchStock
from src.database import get_db_connection, upsert_symbol, upsert_recommendation
from src.utils.disclaimer import append_disclaimer
from src.utils.date_utils import get_kst_now, get_kst_date, get_news_window, KST
from src.market.overnight import fetch_overnight_signals, assess_market_tone
from pytz import UTC

logger = logging.getLogger(__name__)


def filter_by_time_range(news_items: List[NewsItem], 
                         start_dt: datetime, 
                         end_dt: datetime) -> tuple[List[NewsItem], dict]:
    """
    ì‹œê°„ ë²”ìœ„ë¡œ í•„í„°ë§ (ë””ë²„ê·¸ ì •ë³´ í¬í•¨)
    
    Args:
        news_items: ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        start_dt: ì‹œì‘ ë‚ ì§œ/ì‹œê°„ (KST)
        end_dt: ì¢…ë£Œ ë‚ ì§œ/ì‹œê°„ (KST)
    
    Returns:
        (í•„í„°ë§ëœ ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸, ë””ë²„ê·¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬)
    """
    # KSTë¥¼ UTCë¡œ ë³€í™˜ (ë‚´ë¶€ ë¹„êµëŠ” UTCë¡œ)
    if start_dt.tzinfo != KST:
        start_dt = start_dt.astimezone(KST)
    if end_dt.tzinfo != KST:
        end_dt = end_dt.astimezone(KST)
    
    start_dt_utc = start_dt.astimezone(UTC)
    end_dt_utc = end_dt.astimezone(UTC)
    
    filtered = []
    too_old_count = 0
    too_new_count = 0
    no_time_count = 0
    
    for item in news_items:
        if not item.published_at:
            # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì¼ë‹¨ í¬í•¨ (ì •ë ¬ ì‹œ ì•„ë˜ë¡œ)
            no_time_count += 1
            filtered.append(item)
            continue
        
        # UTCë¡œ ë³€í™˜ (ì´ë¯¸ UTCì¼ ìˆ˜ë„ ìˆìŒ)
        item_dt_utc = item.published_at
        if item_dt_utc.tzinfo != UTC:
            item_dt_utc = item_dt_utc.astimezone(UTC)
        
        # ë²”ìœ„ ì²´í¬ (UTCë¡œ ë¹„êµ)
        if item_dt_utc < start_dt_utc:
            too_old_count += 1
            continue
        if item_dt_utc > end_dt_utc:
            too_new_count += 1
            continue
        
        filtered.append(item)
    
    debug_info = {
        "too_old_count": too_old_count,
        "too_new_count": too_new_count,
        "no_time_count": no_time_count,
    }
    
    return (filtered, debug_info)


def generate_morning_report() -> str:
    """
    ì˜¤ì „ ë¦¬í¬íŠ¸ ìƒì„±
    
    Returns:
        ë¦¬í¬íŠ¸ ë©”ì‹œì§€ (Markdown í˜•ì‹)
    """
    now = get_kst_now()
    today = get_kst_date()
    datetime_str = now.strftime("%Y-%m-%d %H:%M KST")
    
    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œê°„ ìœˆë„ìš° ê³„ì‚° (ë¬´ì¡°ê±´ get_news_window ì‚¬ìš©)
    start_dt, end_dt, window_mode, lookback_hours = get_news_window(now, mode=NEWS_WINDOW_MODE)
    
    # 2. ë‰´ìŠ¤ ìˆ˜ì§‘ (ì—ëŸ¬ ì‹œ fallback)
    news_items: List[NewsItem] = []
    fetched_count = 0
    parsed_ok_count = 0
    parsed_fail_count = 0
    
    try:
        # ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
        if GOOGLE_NEWS_QUERIES:
            # ì—¬ëŸ¬ ì¿¼ë¦¬ (ì‰¼í‘œ êµ¬ë¶„)
            queries = [q.strip() for q in GOOGLE_NEWS_QUERIES.split(",") if q.strip()]
        else:
            # ê¸°ë³¸ ì¿¼ë¦¬ ì„¸íŠ¸ ì‚¬ìš©
            queries = DEFAULT_NEWS_QUERIES
        
        news_provider = get_news_provider(
            NEWS_PROVIDER,
            queries=queries,
            max_per_query=GOOGLE_NEWS_MAX_PER_QUERY
        )
        
        # ProviderëŠ” ì‹œê°„ í•„í„°ë§ ì—†ì´ ê°€ëŠ¥í•œ ë§ì´ ê°€ì ¸ì˜´
        news_items = news_provider.fetch_news()
        
        if NEWS_PROVIDER == "rss" and hasattr(news_provider, '_last_fetched_count'):
            fetched_count = news_provider._last_fetched_count
            parsed_ok_count = getattr(news_provider, '_parsed_ok_count', 0)
            parsed_fail_count = getattr(news_provider, '_parsed_fail_count', 0)
        else:
            fetched_count = len(news_items)
            parsed_ok_count = sum(1 for item in news_items if item.published_at)
            parsed_fail_count = sum(1 for item in news_items if not item.published_at)
        
        logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {fetched_count}ê±´ (ì¿¼ë¦¬: {len(queries)}ê°œ)")
        print(f"parsed_ok={parsed_ok_count} parsed_fail={parsed_fail_count}")
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}, ë”ë¯¸ providerë¡œ ì „í™˜", exc_info=True)
        # Fallback: ë”ë¯¸ provider ì‚¬ìš©
        try:
            dummy_provider = DummyNewsProvider()
            news_items = dummy_provider.fetch_news()
            fetched_count = len(news_items)
            parsed_ok_count = sum(1 for item in news_items if item.published_at)
            parsed_fail_count = sum(1 for item in news_items if not item.published_at)
            print(f"parsed_ok={parsed_ok_count} parsed_fail={parsed_fail_count}")
        except Exception as fallback_error:
            logger.error(f"ë”ë¯¸ providerë„ ì‹¤íŒ¨: {fallback_error}")
            # ìµœì¢… fallback: ë¹ˆ ë¦¬í¬íŠ¸
            report = f"*ğŸ“° ì˜¤ì „ ë¦¬í¬íŠ¸ - {today}*\n\n"
            report += f"*ìˆ˜ì§‘ ì‹œê°„:* {datetime_str}\n\n"
            report += "*âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨*\n"
            report += f"ì˜¤ë¥˜: {str(e)}\n\n"
            report = append_disclaimer(report)
            return report
    
    if not news_items:
        report = f"*ğŸ“° ì˜¤ì „ ë¦¬í¬íŠ¸ - {today}*\n\n"
        report += f"*ìˆ˜ì§‘ ì‹œê°„:* {datetime_str}\n\n"
        report += "*ì§€ë‚œë°¤ ì£¼ìš” ë‰´ìŠ¤*\n"
        report += "ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
        report = append_disclaimer(report)
        return report
    
    # 3. ì‹œê°„ í•„í„°ë§ (reportsì—ì„œ ì ìš©)
    time_filtered_items, debug_info = filter_by_time_range(news_items, start_dt, end_dt)
    time_filtered_count = len(time_filtered_items)
    
    print(f"too_old_count={debug_info['too_old_count']} too_new_count={debug_info['too_new_count']} no_time_count={debug_info['no_time_count']}")
    logger.info(f"ì‹œê°„ í•„í„° ({start_dt.strftime('%m/%d %H:%M')} ~ {end_dt.strftime('%m/%d %H:%M')}): {fetched_count}ê±´ â†’ {time_filtered_count}ê±´")
    logger.info(f"íƒˆë½ ì´ìœ : too_old={debug_info['too_old_count']}, too_new={debug_info['too_new_count']}, no_time={debug_info['no_time_count']}")
    
    if not time_filtered_items:
        report = f"*ğŸ“° ì˜¤ì „ ë¦¬í¬íŠ¸ - {today}*\n\n"
        report += f"*ìˆ˜ì§‘ ì‹œê°„:* {datetime_str}\n"
        report += f"*ê¸°ê°„:* {start_dt.strftime('%m/%d %H:%M')} ~ {end_dt.strftime('%m/%d %H:%M')} KST ({window_mode} ëª¨ë“œ)\n"
        report += f"*ìˆ˜ì§‘:* {fetched_count}ê±´ â†’ ì‹œê°„í•„í„°: {time_filtered_count}ê±´\n\n"
        report += "í•´ë‹¹ ì‹œê°„ ë²”ìœ„ì— ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
        report = append_disclaimer(report)
        return report
    
    # 4. published_atì´ Noneì¸ í•­ëª©ì„ ì •ë ¬ ì‹œ ì•„ë˜ë¡œ ë³´ë‚´ê¸°
    time_filtered_items.sort(
        key=lambda x: (x.published_at is None, x.published_at or datetime.min.replace(tzinfo=UTC)),
        reverse=True
    )
    
    # 5. ì˜¤ë²„ë‚˜ì´íŠ¸ ì„ í–‰ ì‹ í˜¸ ìˆ˜ì§‘ (ë‹¤ì´ì œìŠ¤íŠ¸ ìƒì„± ì „ì—)
    overnight_signals = None
    if OVERNIGHT_ENABLED:
        try:
            from datetime import date as date_class
            target_date = date_class.today()
            overnight_signals = fetch_overnight_signals(
                target_date=target_date,
                provider="yahoo",
                tickers=OVERNIGHT_TICKERS,
                debug=OVERNIGHT_DEBUG
            )
        except Exception as e:
            logger.warning(f"ì˜¤ë²„ë‚˜ì´íŠ¸ ì‹ í˜¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}", exc_info=True)
    
    # 6. ë‹¤ì´ì œìŠ¤íŠ¸ ìƒì„± (ì˜¤ë²„ë‚˜ì´íŠ¸ ì‹ í˜¸ ë°˜ì˜)
    digest = create_digest(
        time_filtered_items, 
        fetched_count=fetched_count,
        time_filtered_count=time_filtered_count,
        overnight_signals=overnight_signals
    )
    
    # 7. ì„¹í„°ë³„ ë¶„ë°° ìˆ˜ ë¡œê¹…
    sector_counts = defaultdict(int)
    for item in time_filtered_items:
        from src.analysis.news_analyzer import classify_sector
        sector = classify_sector(item.title, item.content or "")
        sector_counts[sector] += 1
    
    logger.info(f"ì„¹í„°ë³„ ë¶„ë°°: {dict(sector_counts)}")
    
    # 8. DB ì €ì¥ (í–¥í›„ êµ¬í˜„)
    # ì‹¤ì œë¡œëŠ” news, news_symbols í…Œì´ë¸”ì— ì €ì¥
    # ë‚˜ë¨¸ì§€ ë§í¬ëŠ” DBì— ì €ì¥ë§Œ í•˜ê³  ë©”ì‹œì§€ì—ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠìŒ
    
    # 9. ë¦¬í¬íŠ¸ ìƒì„±
    mode_label = "ìš´ì˜" if window_mode == "strict" else "ê°œë°œ"
    report = f"*ğŸ“° ì˜¤ì „ ë¦¬í¬íŠ¸ - {today}*\n\n"
    report += f"*ìˆ˜ì§‘ ì‹œê°„:* {datetime_str}\n"
    report += f"*ëª¨ë“œ:* {window_mode}"
    if window_mode == "now" and lookback_hours:
        report += f" (lookback {lookback_hours}ì‹œê°„)"
    report += "\n"
    report += f"*ê¸°ê°„:* {start_dt.strftime('%m/%d %H:%M')} ~ {end_dt.strftime('%m/%d %H:%M')} KST ({mode_label} ëª¨ë“œ)\n"
    report += f"*ìˆ˜ì§‘:* {digest.fetched_count}ê±´ â†’ ì‹œê°„í•„í„°: {digest.time_filtered_count}ê±´ â†’ ì¤‘ë³µì œê±°: {digest.deduped_count}ê±´"
    
    # ê°œë°œ ëª¨ë“œì—ì„œë§Œ íŒŒì‹± ì •ë³´ í‘œì‹œ
    if window_mode == "now":
        no_time_count = debug_info['no_time_count']
        report += f" (parsed_ok={parsed_ok_count}, parsed_fail={parsed_fail_count}, no_time={no_time_count})"
    report += "\n"
    
    # ê°œë°œ ëª¨ë“œì—ì„œë§Œ í—¤ë“œë¼ì¸ ì„ ì • ë°©ì‹ í‘œì‹œ
    if window_mode == "now":
        report += "*í—¤ë“œë¼ì¸: ì‹œì¥ ê´€ë ¨ë„ ì ìˆ˜ ê¸°ë°˜ ì„ ì •*\n"
    report += "\n"
    
    # í•µì‹¬ í—¤ë“œë¼ì¸ (ìµœëŒ€ 8ê°œ)
    if digest.top_headlines:
        report += "*ğŸ“Œ í•µì‹¬ í—¤ë“œë¼ì¸*\n"
        for i, headline in enumerate(digest.top_headlines[:8], 1):
            # ë””ë²„ê·¸ íƒœê·¸ ì¶”ê°€
            tags = []
            if NEWS_DEBUG_TAGS and digest.headline_debug is not None:
                debug_info = digest.headline_debug.get(headline, {})
                if debug_info.get("freshness_score", 0) > 0.7:
                    tags.append("[FRESH]")
                if debug_info.get("repeat_penalty", 0) > 0.3:
                    tags.append("[REPEAT]")
                if debug_info.get("late_penalty", 0) > 0.2:
                    tags.append("[LATE?]")
            
            tag_str = " " + " ".join(tags) if tags else ""
            report += f"{i}. {headline}{tag_str}\n"
        report += "\n"
    
    # ê±°ì‹œ ìš”ì•½
    if digest.macro_summary:
        report += "*ğŸ“Š ê±°ì‹œ ìš”ì•½*\n"
        report += f"{digest.macro_summary}\n\n"
    
    # ì˜¤ë²„ë‚˜ì´íŠ¸ ì„ í–‰ ì‹ í˜¸ (ì´ë¯¸ ìˆ˜ì§‘ë¨)
    market_tone = None
    if OVERNIGHT_ENABLED and overnight_signals:
        market_tone = assess_market_tone(overnight_signals)
            
            if overnight_signals:
                report += "*ğŸ“ˆ Overnight Signals*\n"
                # ì„±ê³µí•œ ì‹ í˜¸ë§Œ í‘œì‹œ
                successful_signals = [
                    (name, sig) for name, sig in overnight_signals.items()
                    if sig.success and sig.pct_change is not None
                ]
                
                if successful_signals:
                    # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (Nasdaq, S&P500, NVDA, BTC, USDKRW ë“±)
                    priority_order = ["Nasdaq", "S&P500", "NVDA", "BTC", "USDKRW", "US10Y", "EWY", "DXY"]
                    sorted_signals = sorted(
                        successful_signals,
                        key=lambda x: (
                            priority_order.index(x[0]) if x[0] in priority_order else 999,
                            -abs(x[1].pct_change or 0)  # ë³€ë™ë¥  í° ìˆœ
                        )
                    )
                    
                    for name, sig in sorted_signals[:8]:  # ìµœëŒ€ 8ê°œ
                        pct = sig.pct_change
                        emoji = "ğŸ“ˆ" if pct > 0 else "ğŸ“‰" if pct < 0 else "â–"
                        report += f"  {emoji} {name}: {pct:+.1f}%\n"
                    
                    # ì‹œì¥ í†¤ ìš”ì•½
                    tone_emoji = {
                        "risk_on": "ğŸŸ¢",
                        "risk_off": "ğŸ”´",
                        "mixed": "ğŸŸ¡"
                    }
                    tone_label = {
                        "risk_on": "Risk On",
                        "risk_off": "Risk Off",
                        "mixed": "Mixed"
                    }
                    report += f"\n*ì˜¤ëŠ˜ì˜ í†¤: {tone_emoji.get(market_tone, 'âšª')} {tone_label.get(market_tone, 'Unknown')}*\n\n"
                else:
                    report += "  (ì‹ í˜¸ ìˆ˜ì§‘ ì‹¤íŒ¨)\n\n"
            else:
                if OVERNIGHT_DEBUG:
                    report += "*ğŸ“ˆ Overnight Signals*\n"
                    report += "  (ì‹ í˜¸ ìˆ˜ì§‘ ì‹¤íŒ¨)\n\n"
        except Exception as e:
            logger.warning(f"ì˜¤ë²„ë‚˜ì´íŠ¸ ì‹ í˜¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}", exc_info=True)
            if OVERNIGHT_DEBUG:
                report += "*ğŸ“ˆ Overnight Signals*\n"
                report += f"  (ì˜¤ë¥˜: {str(e)})\n\n"
    
    # ì„¹í„°ë³„ ë‰´ìŠ¤
    if digest.sector_bullets:
        report += "*ğŸ·ï¸ ì„¹í„°ë³„ ì£¼ìš” ë‰´ìŠ¤*\n"
        for sector, bullets in list(digest.sector_bullets.items())[:5]:
            report += f"*{sector}*\n"
            for bullet in bullets[:2]:  # ì„¹í„°ë‹¹ ìµœëŒ€ 2ê°œ
                report += f"  â€¢ {bullet}\n"
        report += "\n"
    
    # í•œêµ­ì¥ ì˜í–¥ë„
    report += f"*ğŸ‡°ğŸ‡· í•œêµ­ì¥ ì˜í–¥ë„: {digest.korea_impact}*\n\n"
    
    # ê·¼ê±° ë§í¬ (ìµœëŒ€ 5ê°œë§Œ)
    if digest.sources:
        report += "*ğŸ”— ê·¼ê±° ë§í¬*\n"
        for i, url in enumerate(digest.sources[:5], 1):
            # URLì„ ì§§ê²Œ í‘œì‹œ (ë„ë©”ì¸ë§Œ)
            try:
                from urllib.parse import urlparse
                domain = urlparse(url).netloc
                display_url = domain if domain else url[:50]
            except:
                display_url = url[:50]
            report += f"{i}. [{display_url}]({url})\n"
        report += "\n"
    else:
        # ê·¼ê±° ë§í¬ê°€ ì—†ì„ ë•Œë„ ë¡œê·¸ ì¶œë ¥
        logger.warning("ê·¼ê±° ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤ (sourcesê°€ ë¹„ì–´ìˆìŒ)")
        # ë¦¬í¬íŠ¸ì—ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ (ê¹”ë”í•˜ê²Œ)
    
    # 9. ê´€ì°° ì¢…ëª© ì„ ì • ë° ë¦¬í¬íŠ¸ ì¶”ê°€
    try:
        # LLM ì‚¬ìš© ì—¬ë¶€ ë¡œê·¸
        if LLM_ENABLED:
            logger.info(f"LLM ì‚¬ìš©: model={LLM_MODEL}")
            print(f"[LLM] ì‚¬ìš©: model={LLM_MODEL}")
        else:
            logger.info("LLM ë¹„í™œì„±í™”, ë£° ê¸°ë°˜ ì„ ì • ì‚¬ìš©")
            print("[LLM] ë¹„í™œì„±í™”, ë£° ê¸°ë°˜ ì„ ì • ì‚¬ìš©")
        
        watch_stocks = pick_watch_stocks(
            digest, 
            time_filtered_items, 
            max_count=3, 
            date_str=today,
            overnight_signals=overnight_signals
        )
        
        if watch_stocks:
            report += "*ğŸ‘€ ì˜¤ëŠ˜ì˜ ê´€ì°° ë¦¬ìŠ¤íŠ¸ (êµìœ¡ìš© ì‹œë®¬ë ˆì´ì…˜)*\n\n"
            
            for idx, stock in enumerate(watch_stocks, 1):
                report += f"*{idx}. {stock.name} ({stock.code})*\n"
                report += f"*Thesis:* {stock.thesis}\n\n"
                
                # Catalyst
                report += "*Catalyst:*\n"
                for catalyst in stock.catalysts:
                    report += f"  â€¢ {catalyst}\n"
                report += "\n"
                
                # Risks
                report += "*Risk:*\n"
                for risk in stock.risks:
                    report += f"  â€¢ {risk}\n"
                report += "\n"
                
                # Trigger
                report += f"*ê´€ì°° íŠ¸ë¦¬ê±°:* {stock.trigger}\n\n"
                
                # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ìˆ˜
                report += "*ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ìˆ˜:*\n"
                for item, score in stock.checklist_scores.items():
                    report += f"  â€¢ {item}: {score}/2ì \n"
                report += f"*ì´ì : {stock.total_score}/12ì *\n\n"
                
                # í™•ì‹ ë„
                report += f"*í™•ì‹ ë„: {stock.confidence} - {stock.confidence_reason}*\n\n"
                
                # DB ì €ì¥
                try:
                    symbol_id = upsert_symbol(stock.name, stock.code)
                    upsert_recommendation(
                        date=today,
                        symbol_id=symbol_id,
                        reason=stock.thesis,
                        priority=idx,
                        news_ids=None  # í–¥í›„ êµ¬í˜„
                    )
                except Exception as e:
                    logger.warning(f"ì¢…ëª© {stock.name} DB ì €ì¥ ì‹¤íŒ¨: {e}")
            
            report += "â€» ì¼ë¶€ ì ìˆ˜ëŠ” ì¬ë¬´ë°ì´í„° ì—°ë™ ì „ ê°€ì •ì¹˜ì…ë‹ˆë‹¤\n\n"
        else:
            logger.info("ê´€ì°° ì¢…ëª©ì´ ì„ ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    except Exception as e:
        logger.error(f"ê´€ì°° ì¢…ëª© ì„ ì • ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ë¦¬í¬íŠ¸ëŠ” ê³„ì† ì§„í–‰
    
    # ë©´ì±… ë¬¸êµ¬ ì¶”ê°€
    report = append_disclaimer(report)
    
    return report
